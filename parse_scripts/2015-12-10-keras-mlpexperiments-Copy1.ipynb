{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position(board='         \\n         \\n rnbqkbnr\\n pppp.ppp\\n ........\\n ....p...\\n ....P...\\n .....N..\\n PPPP.PPP\\n RNBQKB.R\\n         \\n         ', score=0, wc=(True, True), bc=(True, True), ep=0, kp=0)\n",
      "(94, 45)\n",
      "(94, 84)\n",
      "(92, 71), N->.\n",
      "Using Theano backend.\n",
      "Couldn't import dot_parser, loading of dot files will not be possible.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from fenparsev4 import *\n",
    "from pybrain.datasets import ClassificationDataSet\n",
    "from pybrain.supervised.trainers import BackpropTrainer\n",
    "from pybrain.tools.shortcuts import buildNetwork\n",
    "from pybrain.structure.modules import TanhLayer\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os\n",
    "from __future__ import print_function\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inizializing read of 75 files...\n",
      "Reading...done\n",
      "Converting to list...done\n",
      "Splitting data...done\n"
     ]
    }
   ],
   "source": [
    "#most important part\n",
    "def fries_ready():\n",
    "    os.system('say your fries are done')\n",
    "    \n",
    "def write(str):\n",
    "    sys.stdout.write('\\r' + str)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "def writeln(str):\n",
    "    sys.stdout.write(str)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "num_files = 75\n",
    "filename_prefix='/media/drive/storage/csv_input/2015-12-08_112mil'\n",
    "filename_suffix_range=range(1,num_files + 1)\n",
    "debug=True\n",
    "    \n",
    "#read in csv\n",
    "df = pd.DataFrame()\n",
    "writeln(\"Inizializing read of %d files...\\n\" % (num_files))\n",
    "for i in filename_suffix_range:\n",
    "    if debug: write(\"Reading...%d/%d\" % (i, num_files))\n",
    "    df = df.append(pd.read_csv(filename_prefix + str(i)))\n",
    "write(\"Reading...done\\n\")\n",
    "#clean columns\n",
    "df['y'] = df['y'].astype(int)\n",
    "if debug: writeln(\"Converting to list...\")\n",
    "df['x'] = df['x'] = df.loc[:, 'x'].apply(lambda x: [1 if '1' == a else 0 for a in x.split(', ')])\n",
    "length = df.shape[0]\n",
    "df = df.set_index([range(0,length)])\n",
    "writeln(\"done\\nShuffling data...\")\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "writeln(\"done\")\n",
    "write(\"Splitting data...\")\n",
    "split = df.shape[0] * 4 / 5\n",
    "all_train = df.iloc[:split, :]\n",
    "all_test = df.iloc[split:, :]\n",
    "writeln(\"done\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#takes in full dataframe and converts to usable dataset\n",
    "def build_dataset(all_train, all_test, nb_classes=2, debug=True):\n",
    "    X_train = list(all_train['x'])\n",
    "    X_test = list(all_test['x'])\n",
    "    if debug: print(\"building y labels\")\n",
    "    y_train = [[1] if y == 1 else [0] for y in all_train['y']]\n",
    "    Y_test_binary = [1 if y == 1 else 0 for y in all_test['y']]\n",
    "    if debug: print(\"converting X_train and X_test to nparrays\")\n",
    "    X_train = np.array(X_train)\n",
    "    X_test = np.array(X_test)\n",
    "    if debug: print(\"converting y labels to categorical\")\n",
    "    # convert class vectors to binary class matrices\n",
    "    Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "    Y_test = np_utils.to_categorical(Y_test_binary, nb_classes)\n",
    "    return (X_train, Y_train, X_test, Y_test, Y_test_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building y labels\n",
      "converting X_train and X_test to nparrays\n",
      "converting y labels to categorical\n"
     ]
    }
   ],
   "source": [
    "# y test binary is a binary vector\n",
    "# Y_test is categorical numpy format\n",
    "(X_train, Y_train, X_test, Y_test, Y_test_binary) = build_dataset(all_train, all_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getActivation(activation):\n",
    "    if (activation == 'tanh'):\n",
    "        return Activation('tanh')\n",
    "    elif (activation == 'sigmoid'):\n",
    "        return Activation('sigmoid')\n",
    "    elif (activation == 'hard_sigmoid'):\n",
    "        return Activation('hard_sigmoid')\n",
    "    else:\n",
    "        print(\"invalid activation!\")\n",
    "\n",
    "def buildMLP(activation='tanh',depth=3, width=512):\n",
    "    if depth < 2:\n",
    "        depth = 2\n",
    "    model = Sequential()\n",
    "    model.add(Dense(width, input_shape=(1536,)))\n",
    "    model.add(getActivation(activation))\n",
    "    model.add(Dropout(0.2))\n",
    "    for i in range(0, depth - 2):\n",
    "        model.add(Dense(width))\n",
    "        model.add(getActivation(activation))\n",
    "        model.add(Dropout(0.2))\n",
    "       \n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    rms = RMSprop()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=rms)\n",
    "    #print(model.to_json())\n",
    "    writeln(\"Model with depth %d built...\" % depth)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class KerasExperiment:\n",
    "    def __init__(self, model, test_df, X_train, Y_train, X_test, Y_test, Y_test_binary, epochs=5, repeat_number = 0, verbose=True):\n",
    "        self.model = model\n",
    "        self.test_df = test_df;\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        self.X_test = X_test\n",
    "        self.Y_test = Y_test\n",
    "        self.Y_test_binary = Y_test_binary\n",
    "        self.nb_epoch = epochs\n",
    "        self.repeat_number = repeat_number\n",
    "\n",
    "    #adds specific piece confusion matrices to results dict d\n",
    "    def add_piece_specifics(self, predicted_y):\n",
    "        #append column for predicted\n",
    "        test_df_result = self.test_df.copy()\n",
    "        test_df_result.loc[:,'predicted'] = predicted_y\n",
    "        \n",
    "        #create new row for piece results\n",
    "        d = {}\n",
    "        #calculate each piece confusion matrix\n",
    "        for p in \"pPrRnNbBqQkK\":\n",
    "            specific_piece = test_df_result[test_df_result['piece_moved'] == p]\n",
    "            cm = [confusion_matrix(specific_piece['y'], specific_piece['predicted'])]\n",
    "            #append column\n",
    "            d[p + '_cm'] = cm\n",
    "        #get white and black\n",
    "        color = test_df_result[test_df_result['piece_moved'].isin(list(\"PRNBQK\"))]\n",
    "        d[\"white_cm\"] = [confusion_matrix(color['y'], color['predicted'])]\n",
    "        color = test_df_result[test_df_result['piece_moved'].isin(list(\"prnbqk\"))]\n",
    "        d[\"black_cm\"] = [confusion_matrix(color['y'], color['predicted'])]\n",
    "        \n",
    "        return pd.DataFrame(d, columns = [a + \"_cm\" for a in \"pPrRnNbBqQkK\"] + [\"white_cm\", \"black_cm\"], index=[1]) \n",
    "    \n",
    "    def run_experiment(self):\n",
    "        self.model.fit(self.X_train, self.Y_train, nb_epoch=self.nb_epoch,\n",
    "                  show_accuracy=True, verbose=2,\n",
    "                  validation_data=(self.X_test, self.Y_test))\n",
    "        score = self.model.evaluate(self.X_test, self.Y_test,\n",
    "                               show_accuracy=True, verbose=0)\n",
    "        predicted_y = self.model.predict_classes(self.X_test, batch_size=32)\n",
    "        cm_overall = [confusion_matrix(predicted_y, self.Y_test_binary)]\n",
    "        #create results row \n",
    "        results_row = pd.DataFrame({\"training_size\" : len(self.X_train), \\\n",
    "                                    \"test_size\" : len(self.Y_test), \\\n",
    "                                      \"pct_white\" : sum(self.Y_test_binary) * 1.0 / len(self.Y_test_binary), \\\n",
    "                                      \"cm_overall\": cm_overall, \\\n",
    "                                   \"epochs\": self.nb_epoch, \\\n",
    "                                   \"network\" : self.model.to_json(),\\\n",
    "                                   \"repeat_number\" : self.repeat_number}, index=[1]);\n",
    "        results_row = results_row.join(self.add_piece_specifics(predicted_y))\n",
    "        return(results_row)\n",
    "        \n",
    "    #    print(confusion_matrix(y_train, out))\n",
    "        #return pd.DataFrame({\"train_size\": self.train_df.shape[0], \n",
    "#                             \"train_white_count\" : sum([1 if a.isupper() else 0 for a in self.train_df['piece_moved']]),\n",
    "#                             \"confusion_matrix\" : [cm],\n",
    "#                             \"accuracy\": [(cm[0][0] + cm[1][1]) * 1.0 / (sum([sum(c) for c in cm]))]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5)\n",
      "Model with depth 3 built...(3, 25)\n",
      "Model with depth 3 built...(3, 50)\n",
      "Model with depth 3 built...(3, 75)\n",
      "Model with depth 3 built...(4, 5)\n",
      "Model with depth 4 built...(4, 25)\n",
      "Model with depth 4 built...(4, 50)\n",
      "Model with depth 4 built...(4, 75)\n",
      "Model with depth 4 built...(5, 5)\n",
      "Model with depth 5 built...(5, 25)\n",
      "Model with depth 5 built...(5, 50)\n",
      "Model with depth 5 built...(5, 75)\n",
      "Model with depth 5 built...(6, 5)\n",
      "Model with depth 6 built...(6, 25)\n",
      "Model with depth 6 built...(6, 50)\n",
      "Model with depth 6 built...(6, 75)\n",
      "Model with depth 6 built..."
     ]
    }
   ],
   "source": [
    "#testing different epoch counts\n",
    "mlp_layer_set = [3, 4, 5, 6]\n",
    "mlp_width_set = [5, 25, 50, 75]\n",
    "test_size = 10000\n",
    "train_size = 100000\n",
    "e_list = []\n",
    "for (mlp_layers, mlp_width) in [(a,b) for a in mlp_layer_set\\\n",
    "                        for b in mlp_width_set]:\n",
    "    print((mlp_layers, mlp_width))\n",
    "    e = KerasExperiment(buildMLP(activation='tanh', depth=mlp_layers, width=mlp_width), all_test.iloc[:test_size,:], X_train[:train_size], Y_train[:train_size],\\\n",
    "                         X_test[:test_size], Y_test[:test_size], Y_test_binary[:test_size])\n",
    "    e_list.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "2s - loss: 0.6777 - acc: 0.5643 - val_loss: 0.6447 - val_acc: 0.6215\n",
      "Epoch 2/5\n",
      "2s - loss: 0.6242 - acc: 0.6427 - val_loss: 0.6182 - val_acc: 0.6243\n",
      "Epoch 3/5\n",
      "2s - loss: 0.6015 - acc: 0.6662 - val_loss: 0.5985 - val_acc: 0.6545\n",
      "Epoch 4/5\n",
      "2s - loss: 0.5896 - acc: 0.6797 - val_loss: 0.5863 - val_acc: 0.6675\n",
      "Epoch 5/5\n",
      "2s - loss: 0.5809 - acc: 0.6870 - val_loss: 0.6004 - val_acc: 0.6514\n",
      "10000/10000 [==============================] - 0s     \n",
      "Train on 100000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "2s - loss: 0.6752 - acc: 0.5674 - val_loss: 0.6558 - val_acc: 0.6034\n",
      "Epoch 2/5\n",
      "2s - loss: 0.6173 - acc: 0.6461 - val_loss: 0.6077 - val_acc: 0.6537\n",
      "Epoch 3/5\n",
      "2s - loss: 0.5949 - acc: 0.6687 - val_loss: 0.6127 - val_acc: 0.6536\n",
      "Epoch 4/5\n",
      "2s - loss: 0.5832 - acc: 0.6815 - val_loss: 0.5782 - val_acc: 0.6873\n",
      "Epoch 5/5\n",
      "2s - loss: 0.5742 - acc: 0.6908 - val_loss: 0.6015 - val_acc: 0.6741\n",
      "10000/10000 [==============================] - 0s     \n",
      "Train on 100000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "3s - loss: 0.6784 - acc: 0.5617 - val_loss: 0.6403 - val_acc: 0.6113\n",
      "Epoch 2/5\n",
      "4s - loss: 0.6187 - acc: 0.6450 - val_loss: 0.5951 - val_acc: 0.6677\n",
      "Epoch 3/5\n",
      "4s - loss: 0.5945 - acc: 0.6696 - val_loss: 0.5882 - val_acc: 0.6752\n",
      "Epoch 4/5\n",
      "4s - loss: 0.5808 - acc: 0.6840 - val_loss: 0.5830 - val_acc: 0.6787\n",
      "Epoch 5/5\n",
      "4s - loss: 0.5732 - acc: 0.6920 - val_loss: 0.5967 - val_acc: 0.6594\n",
      "10000/10000 [==============================] - 0s     \n",
      "Train on 100000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "5s - loss: 0.6757 - acc: 0.5665 - val_loss: 0.6393 - val_acc: 0.6185\n",
      "Epoch 2/5\n",
      "5s - loss: 0.6162 - acc: 0.6478 - val_loss: 0.6015 - val_acc: 0.6547\n",
      "Epoch 3/5\n",
      "5s - loss: 0.5933 - acc: 0.6707 - val_loss: 0.6166 - val_acc: 0.6528\n",
      "Epoch 4/5\n",
      "5s - loss: 0.5809 - acc: 0.6851 - val_loss: 0.5775 - val_acc: 0.6846\n",
      "Epoch 5/5\n",
      "5s - loss: 0.5724 - acc: 0.6921 - val_loss: 0.5646 - val_acc: 0.6952\n",
      "10000/10000 [==============================] - 0s     \n",
      "Train on 100000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "2s - loss: 0.6874 - acc: 0.5420 - val_loss: 0.6601 - val_acc: 0.6127\n",
      "Epoch 2/5\n",
      "2s - loss: 0.6401 - acc: 0.6279 - val_loss: 0.6233 - val_acc: 0.6183\n",
      "Epoch 3/5\n",
      "2s - loss: 0.6123 - acc: 0.6585 - val_loss: 0.5938 - val_acc: 0.6656\n",
      "Epoch 4/5\n",
      "2s - loss: 0.5983 - acc: 0.6747 - val_loss: 0.5848 - val_acc: 0.6767\n",
      "Epoch 5/5\n",
      "2s - loss: 0.5903 - acc: 0.6824 - val_loss: 0.6016 - val_acc: 0.6541\n",
      "10000/10000 [==============================] - 0s     \n",
      "Train on 100000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "3s - loss: 0.6882 - acc: 0.5394 - val_loss: 0.6699 - val_acc: 0.5811\n",
      "Epoch 2/5\n",
      "3s - loss: 0.6299 - acc: 0.6340 - val_loss: 0.6080 - val_acc: 0.6514\n",
      "Epoch 3/5\n",
      "3s - loss: 0.6013 - acc: 0.6652 - val_loss: 0.5862 - val_acc: 0.6767\n",
      "Epoch 4/5\n",
      "3s - loss: 0.5872 - acc: 0.6813 - val_loss: 0.5766 - val_acc: 0.6955\n",
      "Epoch 5/5\n",
      "3s - loss: 0.5779 - acc: 0.6897 - val_loss: 0.5738 - val_acc: 0.6943\n",
      "10000/10000 [==============================] - 0s     \n",
      "Train on 100000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "4s - loss: 0.6851 - acc: 0.5505 - val_loss: 0.6515 - val_acc: 0.6000\n",
      "Epoch 2/5\n",
      "4s - loss: 0.6255 - acc: 0.6384 - val_loss: 0.6016 - val_acc: 0.6542\n",
      "Epoch 3/5\n",
      "4s - loss: 0.5990 - acc: 0.6656 - val_loss: 0.5875 - val_acc: 0.6798\n",
      "Epoch 4/5\n",
      "4s - loss: 0.5834 - acc: 0.6812 - val_loss: 0.5806 - val_acc: 0.6808\n",
      "Epoch 5/5\n",
      "4s - loss: 0.5696 - acc: 0.6918 - val_loss: 0.5919 - val_acc: 0.6761\n",
      "10000/10000 [==============================] - 0s     \n",
      "Train on 100000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "5s - loss: 0.6826 - acc: 0.5535 - val_loss: 0.6423 - val_acc: 0.6329\n",
      "Epoch 2/5\n",
      "5s - loss: 0.6231 - acc: 0.6402 - val_loss: 0.6046 - val_acc: 0.6593\n",
      "Epoch 3/5\n",
      "5s - loss: 0.5965 - acc: 0.6683 - val_loss: 0.5758 - val_acc: 0.6889\n",
      "Epoch 4/5\n",
      "5s - loss: 0.5799 - acc: 0.6810 - val_loss: 0.6008 - val_acc: 0.6627\n",
      "Epoch 5/5\n",
      "5s - loss: 0.5650 - acc: 0.6926 - val_loss: 0.5761 - val_acc: 0.6794\n",
      "10000/10000 [==============================] - 0s     \n",
      "Train on 100000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "2s - loss: 0.6903 - acc: 0.5305 - val_loss: 0.6721 - val_acc: 0.5907\n",
      "Epoch 2/5\n",
      "2s - loss: 0.6511 - acc: 0.6174 - val_loss: 0.6457 - val_acc: 0.6119\n",
      "Epoch 3/5\n",
      "2s - loss: 0.6196 - acc: 0.6554 - val_loss: 0.6029 - val_acc: 0.6627\n",
      "Epoch 4/5\n",
      "2s - loss: 0.6041 - acc: 0.6723 - val_loss: 0.5823 - val_acc: 0.6917\n",
      "Epoch 5/5\n",
      "2s - loss: 0.5933 - acc: 0.6832 - val_loss: 0.5787 - val_acc: 0.6899\n",
      "10000/10000 [==============================] - 0s     \n",
      "Train on 100000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "3s - loss: 0.6875 - acc: 0.5410 - val_loss: 0.6526 - val_acc: 0.6060\n",
      "Epoch 2/5\n",
      "3s - loss: 0.6336 - acc: 0.6342 - val_loss: 0.6164 - val_acc: 0.6457\n",
      "Epoch 3/5\n",
      "3s - loss: 0.6043 - acc: 0.6627 - val_loss: 0.5935 - val_acc: 0.6634\n",
      "Epoch 4/5\n",
      "3s - loss: 0.5873 - acc: 0.6804 - val_loss: 0.5897 - val_acc: 0.6788\n",
      "Epoch 5/5\n",
      "3s - loss: 0.5725 - acc: 0.6903 - val_loss: 0.5788 - val_acc: 0.6831\n",
      "10000/10000 [==============================] - 0s     \n",
      "Train on 100000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "5s - loss: 0.6895 - acc: 0.5390 - val_loss: 0.6497 - val_acc: 0.6202\n",
      "Epoch 2/5\n",
      "5s - loss: 0.6312 - acc: 0.6351 - val_loss: 0.6194 - val_acc: 0.6438\n",
      "Epoch 3/5\n",
      "5s - loss: 0.6002 - acc: 0.6647 - val_loss: 0.6004 - val_acc: 0.6633\n",
      "Epoch 4/5\n",
      "5s - loss: 0.5804 - acc: 0.6807 - val_loss: 0.6027 - val_acc: 0.6629\n",
      "Epoch 5/5\n",
      "5s - loss: 0.5673 - acc: 0.6893 - val_loss: 0.5704 - val_acc: 0.6906\n",
      "10000/10000 [==============================] - 0s     \n",
      "Train on 100000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "6s - loss: 0.6910 - acc: 0.5381 - val_loss: 0.6610 - val_acc: 0.5875\n",
      "Epoch 2/5\n",
      "7s - loss: 0.6336 - acc: 0.6326 - val_loss: 0.6184 - val_acc: 0.6410\n",
      "Epoch 3/5\n",
      "6s - loss: 0.6004 - acc: 0.6639 - val_loss: 0.6131 - val_acc: 0.6519\n",
      "Epoch 4/5\n",
      "6s - loss: 0.5801 - acc: 0.6794 - val_loss: 0.5658 - val_acc: 0.6903\n",
      "Epoch 5/5\n",
      "6s - loss: 0.5641 - acc: 0.6921 - val_loss: 0.6111 - val_acc: 0.6548\n",
      "10000/10000 [==============================] - 0s     \n",
      "Train on 100000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "2s - loss: 0.6927 - acc: 0.5155 - val_loss: 0.6863 - val_acc: 0.5598\n",
      "Epoch 2/5\n",
      "2s - loss: 0.6673 - acc: 0.5941 - val_loss: 0.6326 - val_acc: 0.6352\n",
      "Epoch 3/5\n",
      "2s - loss: 0.6305 - acc: 0.6473 - val_loss: 0.6090 - val_acc: 0.6541\n",
      "Epoch 4/5\n",
      "2s - loss: 0.6104 - acc: 0.6689 - val_loss: 0.6016 - val_acc: 0.6584\n",
      "Epoch 5/5\n",
      "2s - loss: 0.5989 - acc: 0.6813 - val_loss: 0.5867 - val_acc: 0.6838\n",
      "10000/10000 [==============================] - 0s     \n",
      "Train on 100000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "3s - loss: 0.6950 - acc: 0.5200 - val_loss: 0.6754 - val_acc: 0.5650\n",
      "Epoch 2/5\n",
      "3s - loss: 0.6490 - acc: 0.6171 - val_loss: 0.6159 - val_acc: 0.6406\n",
      "Epoch 3/5\n",
      "3s - loss: 0.6126 - acc: 0.6593 - val_loss: 0.5880 - val_acc: 0.6785\n",
      "Epoch 4/5\n",
      "3s - loss: 0.5942 - acc: 0.6779 - val_loss: 0.6087 - val_acc: 0.6481\n",
      "Epoch 5/5\n",
      "3s - loss: 0.5802 - acc: 0.6894 - val_loss: 0.5735 - val_acc: 0.6878\n",
      "10000/10000 [==============================] - 0s     \n",
      "Train on 100000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "5s - loss: 0.6936 - acc: 0.5299 - val_loss: 0.6648 - val_acc: 0.5940\n",
      "Epoch 2/5\n",
      "5s - loss: 0.6388 - acc: 0.6296 - val_loss: 0.6254 - val_acc: 0.6366\n",
      "Epoch 3/5\n",
      "5s - loss: 0.6041 - acc: 0.6646 - val_loss: 0.5834 - val_acc: 0.6751\n",
      "Epoch 4/5\n",
      "5s - loss: 0.5836 - acc: 0.6794 - val_loss: 0.5959 - val_acc: 0.6651\n",
      "Epoch 5/5\n",
      "5s - loss: 0.5705 - acc: 0.6895 - val_loss: 0.5542 - val_acc: 0.7029\n",
      "10000/10000 [==============================] - 0s     \n",
      "Train on 100000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "7s - loss: 0.6962 - acc: 0.5226 - val_loss: 0.6700 - val_acc: 0.5817\n",
      "Epoch 2/5\n",
      "7s - loss: 0.6407 - acc: 0.6264 - val_loss: 0.6418 - val_acc: 0.6049\n",
      "Epoch 3/5\n",
      "7s - loss: 0.6031 - acc: 0.6621 - val_loss: 0.6210 - val_acc: 0.6527\n",
      "Epoch 4/5\n",
      "7s - loss: 0.5843 - acc: 0.6770 - val_loss: 0.5665 - val_acc: 0.6941\n",
      "Epoch 5/5\n",
      "7s - loss: 0.5685 - acc: 0.6885 - val_loss: 0.5519 - val_acc: 0.7053\n",
      "10000/10000 [==============================] - 0s     \n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame()\n",
    "count = 0\n",
    "for e in e_list:\n",
    "    results_df = results_df.append(e.run_experiment())\n",
    "    count += 1\n",
    "    if (count % 5 == 0):\n",
    "        pickle.dump(results_df, open(\"2015-12-12-mlpexperiments_results8.p\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with depth 2 built...Train on 100000 samples, validate on 10000 samples\n",
      "Epoch 1/15\n",
      "18s - loss: 0.6715 - acc: 0.5821 - val_loss: 0.6337 - val_acc: 0.6223\n",
      "Epoch 2/15\n",
      "17s - loss: 0.6163 - acc: 0.6493 - val_loss: 0.6073 - val_acc: 0.6409\n",
      "Epoch 3/15\n",
      "17s - loss: 0.5930 - acc: 0.6714 - val_loss: 0.7060 - val_acc: 0.5564\n",
      "Epoch 4/15\n",
      "17s - loss: 0.5810 - acc: 0.6855 - val_loss: 0.6154 - val_acc: 0.6453\n",
      "Epoch 5/15\n",
      "17s - loss: 0.5712 - acc: 0.6939 - val_loss: 0.5616 - val_acc: 0.7033\n",
      "Epoch 6/15\n",
      "17s - loss: 0.5666 - acc: 0.6993 - val_loss: 0.5622 - val_acc: 0.6976\n",
      "Epoch 7/15\n",
      "17s - loss: 0.5614 - acc: 0.7053 - val_loss: 0.5940 - val_acc: 0.6496\n",
      "Epoch 8/15\n",
      "17s - loss: 0.5575 - acc: 0.7078 - val_loss: 0.5726 - val_acc: 0.7002\n",
      "Epoch 9/15\n",
      "18s - loss: 0.5543 - acc: 0.7127 - val_loss: 0.6274 - val_acc: 0.6776\n",
      "Epoch 10/15\n",
      "18s - loss: 0.5526 - acc: 0.7133 - val_loss: 0.5511 - val_acc: 0.7137\n",
      "Epoch 11/15\n",
      "17s - loss: 0.5488 - acc: 0.7198 - val_loss: 0.5671 - val_acc: 0.6952\n",
      "Epoch 12/15\n",
      "17s - loss: 0.5476 - acc: 0.7192 - val_loss: 0.5761 - val_acc: 0.6813\n",
      "Epoch 13/15\n",
      "17s - loss: 0.5450 - acc: 0.7228 - val_loss: 0.5686 - val_acc: 0.6905\n",
      "Epoch 14/15\n",
      "18s - loss: 0.5429 - acc: 0.7239 - val_loss: 0.5635 - val_acc: 0.7164\n",
      "Epoch 15/15\n",
      "17s - loss: 0.5427 - acc: 0.7245 - val_loss: 0.5791 - val_acc: 0.6930\n",
      "10000/10000 [==============================] - 0s     \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cm_overall</th>\n",
       "      <th>epochs</th>\n",
       "      <th>network</th>\n",
       "      <th>pct_white</th>\n",
       "      <th>test_size</th>\n",
       "      <th>training_size</th>\n",
       "      <th>p_cm</th>\n",
       "      <th>P_cm</th>\n",
       "      <th>r_cm</th>\n",
       "      <th>R_cm</th>\n",
       "      <th>n_cm</th>\n",
       "      <th>N_cm</th>\n",
       "      <th>b_cm</th>\n",
       "      <th>B_cm</th>\n",
       "      <th>q_cm</th>\n",
       "      <th>Q_cm</th>\n",
       "      <th>k_cm</th>\n",
       "      <th>K_cm</th>\n",
       "      <th>white_cm</th>\n",
       "      <th>black_cm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[3191, 1289], [1781, 3739]]</td>\n",
       "      <td>15</td>\n",
       "      <td>{\"layers\": [{\"b_constraint\": null, \"name\": \"De...</td>\n",
       "      <td>0.5028</td>\n",
       "      <td>10000</td>\n",
       "      <td>100000</td>\n",
       "      <td>[[505, 98], [113, 457]]</td>\n",
       "      <td>[[328, 308], [34, 589]]</td>\n",
       "      <td>[[291, 136], [228, 217]]</td>\n",
       "      <td>[[268, 210], [104, 420]]</td>\n",
       "      <td>[[248, 57], [188, 128]]</td>\n",
       "      <td>[[205, 198], [30, 364]]</td>\n",
       "      <td>[[244, 62], [170, 132]]</td>\n",
       "      <td>[[205, 182], [33, 359]]</td>\n",
       "      <td>[[302, 66], [136, 213]]</td>\n",
       "      <td>[[210, 206], [49, 421]]</td>\n",
       "      <td>[[196, 92], [127, 157]]</td>\n",
       "      <td>[[189, 166], [77, 282]]</td>\n",
       "      <td>[[1405, 1270], [327, 2435]]</td>\n",
       "      <td>[[1786, 511], [962, 1304]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     cm_overall  epochs  \\\n",
       "1  [[3191, 1289], [1781, 3739]]      15   \n",
       "\n",
       "                                             network  pct_white  test_size  \\\n",
       "1  {\"layers\": [{\"b_constraint\": null, \"name\": \"De...     0.5028      10000   \n",
       "\n",
       "   training_size                     p_cm                     P_cm  \\\n",
       "1         100000  [[505, 98], [113, 457]]  [[328, 308], [34, 589]]   \n",
       "\n",
       "                       r_cm                      R_cm  \\\n",
       "1  [[291, 136], [228, 217]]  [[268, 210], [104, 420]]   \n",
       "\n",
       "                      n_cm                     N_cm                     b_cm  \\\n",
       "1  [[248, 57], [188, 128]]  [[205, 198], [30, 364]]  [[244, 62], [170, 132]]   \n",
       "\n",
       "                      B_cm                     q_cm                     Q_cm  \\\n",
       "1  [[205, 182], [33, 359]]  [[302, 66], [136, 213]]  [[210, 206], [49, 421]]   \n",
       "\n",
       "                      k_cm                     K_cm  \\\n",
       "1  [[196, 92], [127, 157]]  [[189, 166], [77, 282]]   \n",
       "\n",
       "                      white_cm                    black_cm  \n",
       "1  [[1405, 1270], [327, 2435]]  [[1786, 511], [962, 1304]]  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing different epoch counts\n",
    "mlp_layer_set = [3, 4, 5, 6]\n",
    "mlp_width_set = [5, 25, 50, 75]\n",
    "test_size = 10000\n",
    "train_size = 100000\n",
    "e = KerasExperiment(buildMLP(activation='tanh', depth=2), all_test.iloc[:test_size,:], X_train[:train_size], Y_train[:train_size],\\\n",
    "                         X_test[:test_size], Y_test[:test_size], Y_test_binary[:test_size], epochs=15)\n",
    "e.run_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json.loads(json_str)['layers'][1]['activation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#methods to decode serialized network json\n",
    "import json\n",
    "\n",
    "def get_num_layers(json_str):\n",
    "    # number of actual layers - 5 for input and output / 3 for each hidden + 2 for input and output\n",
    "    return (len(json.loads(json_str)['layers']) - 5) / 3 + 2\n",
    "\n",
    "def get_first_activation(json_str):\n",
    "    return json.loads(json_str)['layers'][1]['activation']\n",
    "\n",
    "def get_first_width(json_str):\n",
    "    return json.loads(json_str)['layers'][0]['output_dim']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_first_width(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results_df.iloc[:, -14:].applymap(lambda x : (x[0][0] + x[1][1]) * 1.0 / sum([sum(a) for a in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_df.to_csv(\"2015-12-11-mlpexperiments_results1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump( results_df, open( \"2015-12-11-mlpexperiments_results1.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pickle.load( open( \"2015-12-11-mlpexperiments_results1.p\", \"rb\" ) )\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'cm_overall', u'epochs', u'network', u'pct_white', u'test_size',\n",
       "       u'training_size', u'p_cm', u'P_cm', u'r_cm', u'R_cm', u'n_cm', u'N_cm',\n",
       "       u'b_cm', u'B_cm', u'q_cm', u'Q_cm', u'k_cm', u'K_cm', u'white_cm',\n",
       "       u'black_cm'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
